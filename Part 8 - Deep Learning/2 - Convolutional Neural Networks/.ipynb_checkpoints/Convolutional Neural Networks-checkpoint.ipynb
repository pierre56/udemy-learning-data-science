{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Plan of attack\n",
    "\n",
    "- What are Convolutional Neural Networks ?\n",
    "- Step 1 - Convolution Operation\n",
    "- Step 1(b) - ReLU Layer\n",
    "- Step 2 - Pooling\n",
    "- Step 3 - Flattening\n",
    "- Step 4 - Full connection\n",
    "- Summary\n",
    "- Bonus: Softmax & Cross-Entropy\n",
    "\n",
    "---\n",
    "\n",
    "## What are Convolutional Neural Networks ?\n",
    "\n",
    "<img src=\"img/cnn_intro.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "<img src=\"img/cnn_intro_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "CNN input are images and can classify them to put a label ont it.\n",
    "The computer understand input black and white or collored picture like a 2d pr 3d array of pixels.\n",
    "\n",
    "<img src=\"img/cnn_intro_3.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "<img src=\"img/cnn_intro_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "#### To go further : \n",
    "[=> Gradient-based learning applied to document recognition by Yann LeCun (1998)](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1 - Convolution Operation\n",
    "\n",
    "<img src=\"img/conv_function.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "In the convolution operation, there are 3 elements : the input image, the feature detector and the feature map.\n",
    "We calculate the feature map by multiplying parts of the input map and the feature detector (elements by elements and putting  the number of matches).\n",
    "\n",
    "Each step/calculation (Square of 9 cases) it called a stride.\n",
    "\n",
    "<img src=\"img/conv_0.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "<img src=\"img/conv_1.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "...\n",
    "\n",
    "<br>\n",
    "<img src=\"img/conv_2.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "<img src=\"img/conv_3.png\" width=\"400\" height=\"200\">\n",
    "...\n",
    "\n",
    "<br>\n",
    "<img src=\"img/conv_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Doing this actions reduce the size of the image to make it easier and faster to process it.\n",
    "\n",
    "Features are how the machine see elements in the picture. As you can see the feature detector has a pattern and the biggest number found in the feature map is when we found the max number of matches.\n",
    "\n",
    "So at the end we will create many feature maps by using different features detectors (or filters).\n",
    "\n",
    "The convolution operation help us to extract feature from a picture to using a feature detector to create different feature maps.\n",
    "\n",
    "<img src=\"img/conv_5.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "__Example of features__:\n",
    "\n",
    "<img src=\"img/conv_example_1.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "<img src=\"img/conv_example_2.png\" width=\"400\" height=\"200\">\n",
    "<br>\n",
    "<img src=\"img/conv_example_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "#### To go further : \n",
    "[=> Introduction to Convolutional Nerual Networks by Jianxin Wu (2017) ](https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1(b) - ReLU Layer\n",
    "\n",
    "This an additionnal step of the convolution operation. After the previous step we will apply an activation function for example the rectifier.\n",
    "\n",
    "<img src=\"img/reLU_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "The rectifier help us to increase the non-linearity in the neural network because images themselves are highly non-linear (different element like borders, colors, pixels ...). \n",
    "\n",
    "__Example:__\n",
    "\n",
    "<img src=\"img/reLU_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "If we extract features of this image of buildings.\n",
    "\n",
    "<img src=\"img/reLU_3_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "We can see sometime some linearity between colors, like some parts of the image when the folowing colors are white -> grey -> dark or dark -> grey -> white\n",
    "\n",
    "<img src=\"img/reLU_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "The rectifier help us to break this kind of linearity.\n",
    "\n",
    "<img src=\"img/reLU_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "#### To go further : \n",
    "[=> Understanding Convolutional Neural Networks with A Mathematical Model by C.C Jay Kuo (2016)](https://arxiv.org/abs/1609.04112)\n",
    "\n",
    "\n",
    "---\n",
    "## Step 2 - Pooling\n",
    "\n",
    "<img src=\"img/pooling_0.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Pooling help us to detecte the same feature however the element we want to extract is placed in the picture. For example the face of the cheetah above.\n",
    "\n",
    "There are different types of pooling (min, max ...).\n",
    "\n",
    "Let's take the example above and especially our feature map.\n",
    "\n",
    "When we apply max pooling , we are doing like convolution applying it to strides and take the max value in order to get a pooled feature map.\n",
    "\n",
    "<img src=\"img/pooling_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "...\n",
    "<br>\n",
    "\n",
    "<img src=\"img/pooling_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "We are still keeping our features (big numbers) but we are removing not important informations and also avoiding kind of distorsion.\n",
    "We are also reducing the size and the number of parameters coming from our original image\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "<img src=\"img/pooling_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "#### Example:\n",
    "[=> Interactive app - Detailed steps for an image](http://scs.ryerson.ca/~aharley/vis/conv/flat.html)\n",
    "\n",
    "<img src=\"img/pooling_4.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "\n",
    "#### To go further : \n",
    "[=> Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition by Dominik Scherer (2010)](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)\n",
    "\n",
    "\n",
    "---\n",
    "## Step 3 - Flattening\n",
    "\n",
    "From the pooled feature map, we take the elements row by row and put in a column.\n",
    "\n",
    "<img src=\"img/flattening_0.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "The goal is to use these columns as vectors of input for a ANN (Artificial neural network)\n",
    "\n",
    "<img src=\"img/flattening_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "<img src=\"img/flattening_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4 - Full connection\n",
    "\n",
    "<img src=\"img/fc_0.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "After flattening, we get the input values for our artificial neural network. But in this case neurons of each layer are __fully connected__ to each other (no weights with 0 as a value). In the output layer of our ANN, we have a neuron for each category of the classification.\n",
    "\n",
    "<img src=\"img/fc_1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Like a normal ANN, the information from the input values are going to be weighted propagated through synapse and neuron and give a probability for an output value (80% dog). \n",
    "Then the error is calculated using a loss function and the return is used to __adjust the weights but also (that's the difference) the features__.\n",
    "\n",
    "<img src=\"img/fc_2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "What weights to assign for each ouput value ?\n",
    "\n",
    "Depend of the value return at the ouput and the error, when the signal is back propagated the weights and neurons for specific features (element of the picture) to choose are the ones which are discrimanitive or descriptive for a category.\n",
    "\n",
    "<img src=\"img/fc_3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "<img src=\"img/fc_4.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "<img src=\"img/summary_0.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Bonus: Softmax & Cross-Entropy\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Example\n",
    "\n",
    "___\n",
    "\n",
    "### Installation Tips\n",
    "\n",
    "#### Installing Theano\n",
    "\n",
    "Theano is an open source library for fast computations for GPU (graphics, more powerful) and CPU (your computer).\n",
    "\n",
    "```bash\n",
    "pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "```\n",
    "\n",
    "#### Installing Tensorflow\n",
    "\n",
    "Tensorflow is another open source library developped by Google Brain used to develop neural network from scratch.\n",
    "\n",
    "[Install Tensorflow from the website](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html)\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "\n",
    "```\n",
    "\n",
    "#### Installing Keras\n",
    "\n",
    "Open source library to build neural network using few lines or codes.\n",
    "\n",
    "```bash\n",
    "pip install --upgrade keras\n",
    "```\n",
    "___\n",
    "\n",
    "### Dataset \n",
    "\n",
    "Images of cats and dofs :)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "Nothing to do :)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Building CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\")`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=250, epochs=25, validation_steps=2000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 434s 2s/step - loss: 0.6792 - acc: 0.5644 - val_loss: 0.6519 - val_acc: 0.6130\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 1541s 6s/step - loss: 0.6215 - acc: 0.6510 - val_loss: 0.5907 - val_acc: 0.6876\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 846s 3s/step - loss: 0.5864 - acc: 0.6896 - val_loss: 0.5732 - val_acc: 0.7170\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 1148s 5s/step - loss: 0.5565 - acc: 0.7090 - val_loss: 0.5354 - val_acc: 0.7360\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 471s 2s/step - loss: 0.5295 - acc: 0.7293 - val_loss: 0.5216 - val_acc: 0.7522\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 424s 2s/step - loss: 0.5065 - acc: 0.7456 - val_loss: 0.6438 - val_acc: 0.6684\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 448s 2s/step - loss: 0.4904 - acc: 0.7621 - val_loss: 0.4972 - val_acc: 0.7669\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 491s 2s/step - loss: 0.4760 - acc: 0.7707 - val_loss: 0.4894 - val_acc: 0.7654\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 412s 2s/step - loss: 0.4658 - acc: 0.7749 - val_loss: 0.4801 - val_acc: 0.7750\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 418s 2s/step - loss: 0.4426 - acc: 0.7899 - val_loss: 0.4884 - val_acc: 0.7721\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 455s 2s/step - loss: 0.4343 - acc: 0.7937 - val_loss: 0.5241 - val_acc: 0.7461\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 434s 2s/step - loss: 0.4315 - acc: 0.7996 - val_loss: 0.4554 - val_acc: 0.7880\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 419s 2s/step - loss: 0.4130 - acc: 0.8076 - val_loss: 0.5129 - val_acc: 0.7705\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 446s 2s/step - loss: 0.4143 - acc: 0.8065 - val_loss: 0.5067 - val_acc: 0.7465\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 402s 2s/step - loss: 0.4006 - acc: 0.8226 - val_loss: 0.4681 - val_acc: 0.7857\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 403s 2s/step - loss: 0.3844 - acc: 0.8245 - val_loss: 0.4799 - val_acc: 0.7987\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 414s 2s/step - loss: 0.3772 - acc: 0.8310 - val_loss: 0.4687 - val_acc: 0.7878\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 421s 2s/step - loss: 0.3692 - acc: 0.8313 - val_loss: 0.4723 - val_acc: 0.7948\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 414s 2s/step - loss: 0.3633 - acc: 0.8380 - val_loss: 0.5176 - val_acc: 0.7838\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 415s 2s/step - loss: 0.3527 - acc: 0.8425 - val_loss: 0.5178 - val_acc: 0.7783\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 423s 2s/step - loss: 0.3474 - acc: 0.8478 - val_loss: 0.4816 - val_acc: 0.7902\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 432s 2s/step - loss: 0.3447 - acc: 0.8479 - val_loss: 0.5011 - val_acc: 0.7968\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 1040s 4s/step - loss: 0.3335 - acc: 0.8501 - val_loss: 0.4701 - val_acc: 0.7935\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 411s 2s/step - loss: 0.3294 - acc: 0.8559 - val_loss: 0.4936 - val_acc: 0.7951\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 399s 2s/step - loss: 0.3161 - acc: 0.8600 - val_loss: 0.4904 - val_acc: 0.7863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1261fcfd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(output_dim = 128, activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 8000,\n",
    "                         nb_epoch = 25,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
